"""
Checkpoint Inspector - Check what's inside your checkpoint file
Helps diagnose EMA weights and model architecture issues
"""

import torch
import argparse

parser = argparse.ArgumentParser(description='Inspect checkpoint file')
parser.add_argument('--checkpoint', required=True, type=str, help='Path to checkpoint file')
args = parser.parse_args()

print("="*80)
print("CHECKPOINT INSPECTOR")
print("="*80)

try:
    checkpoint = torch.load(args.checkpoint, map_location='cpu', weights_only=False)
except:
    checkpoint = torch.load(args.checkpoint, map_location='cpu')

print(f"\nğŸ“ File: {args.checkpoint}")
print(f"ğŸ“¦ Type: {type(checkpoint)}")

if isinstance(checkpoint, dict):
    print(f"\nğŸ”‘ Keys in checkpoint:")
    for key in checkpoint.keys():
        print(f"  - {key}")
    
    # Check for EMA
    if 'ema_shadow' in checkpoint:
        print(f"\nâœ… EMA WEIGHTS FOUND!")
        print(f"   EMA shadow contains {len(checkpoint['ema_shadow'])} parameters")
        print(f"   ğŸ“Š This checkpoint was trained with --use_ema")
    else:
        print(f"\nâš ï¸  NO EMA WEIGHTS")
        print(f"   This checkpoint was trained WITHOUT --use_ema")
    
    # Check state dict
    if 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
        print(f"\nğŸ“Š Model State Dict:")
        print(f"   Total parameters: {len(state_dict)}")
        
        # Check for model type based on key names
        sample_keys = list(state_dict.keys())[:5]
        print(f"\n   Sample parameter names:")
        for key in sample_keys:
            print(f"     - {key}: {state_dict[key].shape}")
        
        # Detect model type
        if any('patch_embed_small' in key for key in state_dict.keys()):
            print(f"\n   ğŸ·ï¸  Model Type: SMALL (model_S.py)")
            print(f"      Use: --model small")
        else:
            print(f"\n   ğŸ·ï¸  Model Type: FULL (model.py)")
            print(f"      Use: --model full")
    
    # Check training info
    if 'epoch' in checkpoint:
        print(f"\nğŸ“ˆ Training Info:")
        print(f"   Epoch: {checkpoint['epoch']}")
    
    if 'best_psnr' in checkpoint:
        print(f"   Best PSNR: {checkpoint['best_psnr']:.4f} dB")
    
    if 'using_ema' in checkpoint:
        print(f"   Using EMA: {checkpoint['using_ema']}")
        if checkpoint['using_ema']:
            print(f"   âœ… State dict contains EMA weights (ready for test.py)")
    
    # Recommendations
    print(f"\nğŸ’¡ RECOMMENDATIONS:")
    
    if 'ema_shadow' in checkpoint and 'using_ema' not in checkpoint:
        print(f"   âš ï¸  This is an OLD checkpoint with EMA weights")
        print(f"   ğŸ“ Updated test.py will automatically load EMA weights")
        print(f"   âœ… Just run: python test.py --weights {args.checkpoint} --model [small/full]")
    elif 'ema_shadow' in checkpoint and checkpoint.get('using_ema', False):
        print(f"   âœ… This is a NEW checkpoint with EMA in state_dict")
        print(f"   âœ… Run: python test.py --weights {args.checkpoint} --model [small/full]")
    else:
        print(f"   â„¹ï¸  This checkpoint was trained WITHOUT EMA")
        print(f"   âœ… Run: python test.py --weights {args.checkpoint} --model [small/full]")
        print(f"   ğŸ’¡ For better results, retrain with --use_ema")

else:
    print(f"\nâš ï¸  Checkpoint is not a dictionary (old format)")
    print(f"   Trying to count parameters...")
    try:
        print(f"   Total parameters: {len(checkpoint)}")
    except:
        print(f"   Could not determine parameter count")

print(f"\n" + "="*80)
print("INSPECTION COMPLETE")
print("="*80)
